---
title: "Decoding the Texas Housing Market with XGBoost-Powered Predictive Analytics"
author: "Luis Rivera"
bibliography: references/references_mlp1.bib
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 2
    toc-location: body
    number-sections: false
    html-math-method: mathjax
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
import json
import geopandas as gpd
import pandas as pd
import numpy as np
import folium
from folium.plugins import DualMap
# Display the combined maps (e.g., in a Jupyter Notebook or save to HTML file)
from IPython.display import HTML
# libraries for machine learning model and folium map
from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from folium.features import GeoJson, GeoJsonTooltip

```

## Overview

_Leveraging an XGBoost regression model, I accurately predicted average home prices across the state of Texas. Utilizing U.S. Census zipcode boundary data, the results were visualized in a Folium choropleth map. This project provides a comprehensive, step-by-step guide to the modeling process, from data preprocessing to final evaluation._

```{python}
# Displaying my dual map
HTML("ml_datasets/dual_map.html")
```

## Home Prices in Texas

__What is the average home price by Zipcode in Texas?__

_We know house prices continue to skyrocket across the nation, making it more difficult for the rising generation of Americans to afford a home. In Texas, the "median home prices rose by about 40 percent between 2019 and 2023..."[@Comptroller]. We know these overall statistics, but what areas have the highest home prices? And how do we measure the boundaries of an "area"? This information would be really beneficial for those looking to buy a home in the near future somewhere in Texas, whether it be humid Houston, or dry Amarillo. It would be very useful for Texas residents to see the average home price for a particular bounded "area". Better yet, are we able to predict the price of a home in Texas with household data such as land space, square footage, number of bedrooms, etc.? How accurate of a model can we create? In this study, my aim was to develop a model that can confidently predict the price of a home in Texas based on household data. The company Barking Data has provided a dataset on homes for sale across the United States [@BarkingData] that appears to be on homes being sold in the year 2022. It is difficult to come across the kind of information Barking Data so thoughtfully provided. Here is a sample of the household data attributes provided in the dataset:_

- _address_
- _street_name_
- _apartment_
- _city_
- _latitude_
- _longitude_
- _postcode_
- _price_
- _bedroom_number_
- _bathroom_number_
- _price_per_unit_
- _living_space_
- _land_space_
- _property_type_
- _property_status_

_I cleaned up the dataset by first filtering the data to only look at data for the state of Texas (target state). Furthermore, I was interested in properties that were "For Sale". I noticed that there were some homes that had a living space of only "0, 1, 2 or 3 square feet, so I removed this from my dataset. I also filtered my dataset by properties that were: apartments, condos, manufactured homes, multi-family homes, single-family homes, and townhouses. I removed any lots, being sold because I was interested in only homes. Land space cannot be negative, and I noticed some negative landspace so I filtered out the rows with negative landscape values. After using other filtering methods, and one-hot encoding, I ended up with the following dataset (sample shown):_

```{python}

# Here I am reading in my data
df = pd.read_csv("./ml_datasets/US Homes Data (For Sale Properties).csv")

# I filter to only keep data from the state of Texas, specifically for homes that are for sale
df = df[(df["state"].isin(["TX"])) & (df["property_status"].isin(["FOR_SALE"])) & (~df["living_space"].isin([0,1,2,3]))]

# I only look at apartments, condo, manufactured, multi_family, single_family, and townhouse homes
df = df[~df['property_type'].isin(['LOT'])]

# I filter out the rows with negative landscape values
df = df[~df['land_space'].isin([-10890.0])]

# Here I change acres to square feet in the land_space column
df['land_space'] = round(df.apply(lambda row: row['land_space'] * 43560 if row['land_space_unit'] == 'acres' else row['land_space'], axis=1),2)

# Here I make the price_per_unit column more accurate
df['price_per_unit'] = round(df['price'] / df['living_space'], 2)
df['price_per_unit'] = df.apply(lambda row: round(row['price'] / row['living_space'], 2) if (row['living_space'] > 0) and not pd.isna(row['living_space']) else row['price_per_unit'], axis=1)

# Here I drop the extra columns that I do not need, and can drop now
df = df.drop(['property_url','property_id', 'address', 'street_name', 'apartment', 'city', 'state','land_space_unit', 'broker_id', 'property_status',
              'year_build', 'total_num_units' , 'listing_age', 'RunDate', 'agency_name', 'agent_name', 'agent_phone', 'latitude', 'longitude',
              'price_per_unit'], axis=1).reset_index(drop=True)

# One-hot encoding
df = pd.get_dummies(df, columns=['property_type'], dtype='int')

# Here I change the name of my postcode column to zip_code
df.rename(columns={'postcode': 'zip_code'}, inplace=True)

df.head(10)
```

## Machine Learning Preparation
_In order to finish preparing my dataframe for a machine learning model, I used a dataset provided in github that provides the geometric shapes by zipcode[@geometry]. This dataframe was used to extract the centroid latitude and centroid longitude for the machine learning model I ended up using- XGBoost Regression. I ended up using this dataframe later on to extract the geometry column which contains the geometric shapes that I needed to create a folium choropleth map that shows the accuracy of my model in predicting prices of homes._     

```{python}

# Machine learning preparation.
# Turning the city column to a categorical type
# This is important for XGBoost to handle the data correctly
# df['city'] = df['city'].astype('category')
# ml_df = df.copy()

# Load the GeoJSON shape file for Texas ZIP codes
url = "https://raw.githubusercontent.com/OpenDataDE/State-zip-code-GeoJSON/refs/heads/master/tx_texas_zip_codes_geo.min.json"
zip_shapes = gpd.read_file(url)

# Make sure ZIP codes are strings (important for matching)
df['zip_code'] = df['zip_code'].astype(str)
zip_shapes['ZCTA5CE10'] = zip_shapes['ZCTA5CE10'].astype(str)

# Here I do a left join (everything from the ml_df and only matching rows from zip_shapes)
new_df = pd.merge(df,zip_shapes, left_on='zip_code', right_on='ZCTA5CE10', how='left')

# Here I drop the extra columns that I do not need
ml_clean = new_df.drop(['STATEFP10', 'ZCTA5CE10', 'GEOID10','CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10',
                          'AWATER10', 'INTPTLAT10', 'INTPTLON10', 'PARTFLG10'], axis=1)

# Tell GeoPandas which column holds the geometry
ml_clean = gpd.GeoDataFrame(ml_clean, geometry='geometry')

# Check the current coordinate reference system (CRS)
# print(ml_clean.crs)

# Now I can safely extract spatial features

# Extract spatial features
# Calculate centroid coordinates in meters
ml_clean['zip_centroid_lon'] = ml_clean.geometry.centroid.x
ml_clean['zip_centroid_lat'] = ml_clean.geometry.centroid.y

# Here I drop the geometry column (not needed for modeling) and city column (one-hot encoding and label encoding are not ideal for this column)
ml_clean = ml_clean.drop(columns=['geometry'])

zip_shapes.head(10)
```

## Results

_It is important to note that the geometry column gets dropped since this is not needed in the machine learning model. However, as mentioned earlier, before dropping it we derived the centroid longitude and centroid latitude from the geometry column to use in the machine learning model. This column is used again after training my machine learning model in order to demonstrate the accuracy of the results of my machine learning model compared to the test data in a folium choropleth map. Below you will find the clean final dataframe (sample) that was used by my machine learning model . It is important to note that the target variable used for the machine learning model was the log of 1 plus the "price" column (the home "price" column is represented as an "x" in the equation):_

$\text{target column} = \log\left(1+x\right)$

_The reason why I decided to use this as my target column is because the outliers in the price column were causing my folium choropleth legend scale to be heavily skewed, making it unreadable. Therefore, by changing the scale of the target column house price, the legend for each folium choropleth map shows up balanced, making it easy to read._

_The reason why I ended up going with the XGBoost Regression model is because this model can work with features that contain missing data, and we used the regression model of XGBoost because our target variable is continuous rather than categorical. I decided to use a test size of 0.20 and a random state of 42 when splitting my data for training and when I created the instance of my model in order to help with reproducibility of the test._

_Let's go over the evaluation metrics I used for my model. The Mean Squared Error (MSE) measures the average of the squared differences between the predicted and actual values (see equation below):_

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

_This model is sensitive to outliers. We can see from my results below that the MSE was 0.12, which is low._

_The Root Mean Squared Error (RMSE) is the square root of the Mean Squared Error (MSE), which means it brings the units back to those of the target variable (the log scale of the home prices column in this case). This metric provides the average error size. Please see the equation below:_

$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

_We can see that the Root Mean Squared Error (RMSE) is 0.35. Again, the root mean squared error is low._

_The Mean Absolute Error (MAE) measures the average error without considering the direction (negative, positive). Please see equation below:_
$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|
$$

_Our Mean Absolute Error is 0.22, which is low._

Finally we look at the R^2^ score. This measures the proportion of the variance in the dependent variable (target variable) that is predictable from the independent variables. R^2^ values fall within the range 0 to 1 (0% to 100%). The closer the number is to 1 the closer it is to a perfect fit. In other words, the model better explains the variability in the dependent variable (target variable). Please see the equation below:_

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

_We got a result of 0.85 which is overall great._

_Below you will see the dataframe that was fed to the XGBoost model to train and test the model (X- represents the features. y- represents the target variable). You will notice the change in the scale of the target variable to a log scale. Below that you will see the results of the metrics used to evaluate the model as we have discussed._

```python
# Drop zipcode for model training
X = ml_clean.drop(columns=['price'])
y = ml_clean['price']

# Here I make the target variable a log to improve my machine learning model performance
y = np.log1p(y)

X_train_full, X_test_full, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Here I save the zipcodes separately before I drop them for modeling

# Zipcodes aligned to my training set
zip_train = X_train_full['zip_code'].reset_index(drop=True)

# Zipcodes aligned to my test set
zip_test = X_test_full['zip_code'].reset_index(drop=True)

# Here I remove the zipcodes from the actual training data
X_train = X_train_full.drop(columns=['zip_code'])
X_test = X_test_full.drop(columns=['zip_code'])

# create model instance
xgb = XGBRegressor(n_estimators=350, max_depth=10, learning_rate=.01, random_state=42)
# fit model
xgb.fit(X_train, y_train)
# make predictions
y_pred = xgb.predict(X_test)
```

```{python}

# Drop zipcode for model training
X = ml_clean.drop(columns=['price'])
y = ml_clean['price']

# Here I make the target variable a log to improve my machine learning model performance
y = np.log1p(y)

X_train_full, X_test_full, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Here I save the zipcodes separately before I drop them for modeling

# Zipcodes aligned to my training set
zip_train = X_train_full['zip_code'].reset_index(drop=True)

# Zipcodes aligned to my test set
zip_test = X_test_full['zip_code'].reset_index(drop=True)

# Here I remove the zipcodes from the actual training data
X_train = X_train_full.drop(columns=['zip_code'])
X_test = X_test_full.drop(columns=['zip_code'])

# create model instance
xgb = XGBRegressor(n_estimators=350, max_depth=10, learning_rate=.01, random_state=42)
# fit model
xgb.fit(X_train, y_train)
# make predictions
y_pred = xgb.predict(X_test)

print("X-Features:")
X.head(10)
```
```{python}
print("y-Target Variable:")
y.head(10)
```
```{python}

print("Mean Squared Error (MSE):", round(mean_squared_error(y_test, y_pred),2))
rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)),2)
print("Root Mean Squared Error (RMSE):", round(rmse,2))
print("Mean Absolute Error (MAE):", round(mean_absolute_error(y_test, y_pred),2))
print("R² Score:", round(r2_score(y_test, y_pred),2))

# # undo the log1p transformation to get the actual price predictions
# y_pred = np.expm1(y_pred)
# y_test = np.expm1(y_test)

results = pd.DataFrame({
    'zip_code': zip_test,
    'prediction': y_pred,
    'actual_price': y_test.reset_index(drop=True)
})

# results

# Here I group by zip_code column and calculate the mean of the prediction and actual values
results = results.groupby('zip_code')[['prediction', 'actual_price']].mean()

# Round to two decimal places
results['actual_price'] = results['actual_price'].round(2)

# Here I change the name of my predicition column and actual price column
results = results.rename(columns={'prediction': 'average_price_prediction_log', 'actual_price': 'actual_average_price_log'})

results = pd.DataFrame(results).reset_index()

# undo the log1p transformation to get the actual price predictions in different columns
results['average_price_prediction'] = np.expm1(results['average_price_prediction_log'])
results['actual_average_price'] = np.expm1(results['actual_average_price_log'])


# Here I do a left join (everything from the ml_df and only matching rows from zip_shapes)
results = pd.merge(results,zip_shapes, left_on='zip_code', right_on='ZCTA5CE10', how='left')

# Here I drop the extra columns that I do not need
results = results.drop(['STATEFP10', 'ZCTA5CE10', 'GEOID10','CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10',
                          'AWATER10', 'INTPTLAT10', 'INTPTLON10', 'PARTFLG10'], axis=1)

# Tell GeoPandas which column holds the geometry
results = gpd.GeoDataFrame(results, geometry='geometry')

# Simplify geometry to reduce file size (tolerance controls precision)
# Tolerance controls how much simplification is applied. Higher values = more simplification (more vertices removed).
# Lower values = more detailed shape retained. In this case, 0.01 is a relatively small simplification, meaning fine detail is mostly preserved.
# preserve_topology=True ensures that the simplified geometry does not become invalid. Prevents things like self-intersecting polygons, holes merging or disappearing incorrectly, borders of neighboring shapes overlapping or separating when they shouldn’t.
results['geometry'] = results['geometry'].simplify(tolerance=0.01, preserve_topology=True)

# results
```

## Folium Choropleth - Map Visualization (Predicted vs Actual)
_Now lets talk about the results in the folium choropleth map. You will notice that the dataset did not provide all the zipcodes in the state of Texas. This is evident by the missing boundaries. However, there was plenty of data available. After obtaining the results of the test data from the XGBoost model, I needed to aggregate the data by grouping by zipcodes and obtaining the average home price (in log scale) for each zipcode. On the map to the left, you will find the average price prediction by the XGBoost model for each zipcode and on the map to the left you will find the actual average price for each zipcode. As you can see, the model does a great job in predicting average home prices, the results are nearly identical to the actual average prices as shown in the map on the right. Now it is difficult to know what the actual home price is of a home if it is measured in a log scale, which is why the predicted and actual average prices are given normally (converted back to dollars) for each zipcode in each map if you hover over any zipcode in each map. That way, you know the actual average price of a home in dollars for each zipcode._

```{python}
# The actual heat map
# geojson_data = tacos.__geo_interface__  # convert GeoDataFrame to GeoJSON
# Create a map centered over Texas
# width="35%", height="70%"
m1 = folium.Map(location=[31.9686, -99.9018], zoom_start=5.25, tiles="openstreetmap")

# Add the choropleth (color-coded layer)
folium.Choropleth(
    # This is the GeoJSON data that contains the shapes of the ZIP code areas.
    geo_data=results,
    # This is the data that I want to visualize.
    data=results,
    # Here I specify what columns to use (the column that ties to the geographic shapes and the column with the data to visualize).
    # The first column is the ZIP code, and the second column is the average price.
    columns=['zip_code', 'average_price_prediction_log'],
    # The key_on value should be a string that represents the path in the GeoJSON structure to the property (key) that holds the value you want to match with the data DataFrame.
    key_on='feature.properties.zip_code',
    # This sets the color scale used to fill each area.
    fill_color='plasma',
    # This sets the transparency of the filled areas.
    fill_opacity=0.7,
    # This sets the transparency of the boundary lines between the geographic shapes.
    line_opacity=0.45,
    legend_name= "Average Price Prediction (Log Scale- Smallest to Largest)"
).add_to(m1)

tooltip = GeoJson(
    results,
    style_function=lambda x: {
        # Don't add a fill color (we're already coloring with the choropleth)
        'fillColor': 'transparent',
        # Hide the border line color
        'color': 'transparent',
        # No border line thickness
        'weight': 0
    },
    tooltip=GeoJsonTooltip(
        # Columns to display in the tooltip
        fields=['zip_code', 'average_price_prediction'],
        # What to display as labels for the fields (instead of the column names)
        aliases=['ZIP Code:', 'Average Price Prediction:'],
        # Formats numbers using local formatting (e.g. commas in large numbers)
        localize=True,
        # Tooltip "sticks" to your mouse as you move around that shape. Nice UX
        sticky=True,
        # Shows the field names (the aliases you defined).
        labels=True
    )
).add_to(m1)

# The actual heat map
# geojson_data = tacos.__geo_interface__  # convert GeoDataFrame to GeoJSON
# Create a map centered over Texas

# width="35%", height="70%"
m2 = folium.Map(location=[31.9686, -99.9018], zoom_start=5.25, tiles="openstreetmap")

# Add the choropleth (color-coded layer)
folium.Choropleth(
    # This is the GeoJSON data that contains the shapes of the ZIP code areas.
    geo_data=results,
    # This is the data that I want to visualize.
    data=results,
    # Here I specify what columns to use (the column that ties to the geographic shapes and the column with the data to visualize).
    # The first column is the ZIP code, and the second column is the average price.
    columns=['zip_code', 'actual_average_price_log'],
    # The key_on value should be a string that represents the path in the GeoJSON structure to the property (key) that holds the value you want to match with the data DataFrame.
    key_on='feature.properties.zip_code',
    # This sets the color scale used to fill each area.
    fill_color='plasma',
    # This sets the transparency of the filled areas.
    fill_opacity=0.7,
    # This sets the transparency of the boundary lines between the geographic shapes.
    line_opacity=0.45,
    legend_name= "Actual Average Price (Log Scale - Smallest to Largest)"
).add_to(m2)

tooltip = GeoJson(
    results,
    style_function=lambda x: {
        # Don't add a fill color (we're already coloring with the choropleth)
        'fillColor': 'transparent',
        # Hide the border line color
        'color': 'transparent',
        # No border line thickness
        'weight': 0
    },
    tooltip=GeoJsonTooltip(
        # Columns to display in the tooltip
        fields=['zip_code', 'actual_average_price'],
        # What to display as labels for the fields (instead of the column names)
        aliases=['ZIP Code:', 'Actual Average Price:'],
        # Formats numbers using local formatting (e.g. commas in large numbers)
        localize=True,
        # Tooltip "sticks" to your mouse as you move around that shape. Nice UX
        sticky=True,
        # Shows the field names (the aliases you defined).
        labels=True
    )
).add_to(m2)

# Save the maps as separate HTML files
# m1.save("ml_datasets/predicted_price_map.html")
# m2.save("ml_datasets/actual_price_map.html")

# Embed maps side-by-side in HTML
html_string = """
    <div style="display: flex;">
        <div style="width: 50%; height: 100%;">
            <h3>Average Price Prediction (Log Scale)</h3>
            {m1}
        </div>
        <div style="width: 50%; height: 100%;">
            <h3>Actual Average Price (Log Scale)</h3>
            {m2}
        </div>
    </div>
""".format(m1=m1._repr_html_(), m2=m2._repr_html_())

with open("ml_datasets/dual_map.html", "w") as f:
    f.write(html_string)

dual_map = HTML(html_string)

dual_map
```
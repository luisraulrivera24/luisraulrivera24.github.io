[
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Luis Rivera",
    "section": "",
    "text": "303-960-6369 | LuisRaulRivera24@gmail.com | linkedin.com/in/luisrivera98\nBrigham Young University - Idaho | Rexburg, ID\nSep 2018 - Dec 2021\nBrigham Young University - Idaho | Rexburg, ID\nAug 2024 - Present\nNew Jersey Department of Transportation | Ewing, NJ\nJan 2023 - May 2024\nBrigham Young University-Idaho | Rexburg, ID\nApr 2021 - Dec 2021\nBrigham Young University-Idaho | Rexburg, ID\nJan 2019 - Apr 2020\nChurch of Jesus Christ of Latter-day Saints | San Fernando, CA\nAug 2016 - Jul 2018"
  },
  {
    "objectID": "resume.html#areas-of-expertise",
    "href": "resume.html#areas-of-expertise",
    "title": "Luis Rivera",
    "section": "Areas of Expertise",
    "text": "Areas of Expertise"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Luis Rivera",
    "section": "Education",
    "text": "Education"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Luis Rivera",
    "section": "Professional Experience",
    "text": "Professional Experience"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Decoding the Texas Housing Market with XGBoost-Powered Predictive Analytics",
    "section": "",
    "text": "Show the code\nimport json\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport folium\nfrom folium.plugins import DualMap\n# Display the combined maps (e.g., in a Jupyter Notebook or save to HTML file)\nfrom IPython.display import HTML\n# libraries for machine learning model and folium map\nfrom sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom folium.features import GeoJson, GeoJsonTooltip",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html#overview",
    "href": "Machine_Learning/project1.html#overview",
    "title": "Decoding the Texas Housing Market with XGBoost-Powered Predictive Analytics",
    "section": "Overview",
    "text": "Overview\nLeveraging an XGBoost regression model, I accurately predicted average home prices across the state of Texas. Utilizing U.S. Census zipcode boundary data, the results were visualized in a Folium choropleth map. This project provides a comprehensive, step-by-step guide to the modeling process, from data preprocessing to final evaluation.\n\n\nShow the code\n# Displaying my dual map\nHTML(\"ml_datasets/dual_map.html\")\n\n\n\n    \n        \n            Average Price Prediction (Log Scale)\n            Make this Notebook Trusted to load map: File -&gt; Trust Notebook\n        \n        \n            Actual Average Price (Log Scale)\n            Make this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html#home-prices-in-texas",
    "href": "Machine_Learning/project1.html#home-prices-in-texas",
    "title": "Decoding the Texas Housing Market with XGBoost-Powered Predictive Analytics",
    "section": "Home Prices in Texas",
    "text": "Home Prices in Texas\nWhat is the average home price by Zipcode in Texas?\nWe know house prices continue to skyrocket across the nation, making it more difficult for the rising generation of Americans to afford a home. In Texas, the “median home prices rose by about 40 percent between 2019 and 2023…”(Green 2024). We know these overall statistics, but what areas have the highest home prices? And how do we measure the boundaries of an “area”? This information would be really beneficial for those looking to buy a home in the near future somewhere in Texas, whether it be humid Houston, or dry Amarillo. It would be very useful for Texas residents to see the average home price for a particular bounded “area”. Better yet, are we able to predict the price of a home in Texas with household data such as land space, square footage, number of bedrooms, etc.? How accurate of a model can we create? In this study, my aim was to develop a model that can confidently predict the price of a home in Texas based on household data. The company Barking Data has provided a dataset on homes for sale across the United States (BarkingData 2022) that appears to be on homes being sold in the year 2022. It is difficult to come across the kind of information Barking Data so thoughtfully provided. Here is a sample of the household data attributes provided in the dataset:\n\naddress\nstreet_name\napartment\ncity\nlatitude\nlongitude\npostcode\nprice\nbedroom_number\nbathroom_number\nprice_per_unit\nliving_space\nland_space\nproperty_type\nproperty_status\n\nI cleaned up the dataset by first filtering the data to only look at data for the state of Texas (target state). Furthermore, I was interested in properties that were “For Sale”. I noticed that there were some homes that had a living space of only “0, 1, 2 or 3 square feet, so I removed this from my dataset. I also filtered my dataset by properties that were: apartments, condos, manufactured homes, multi-family homes, single-family homes, and townhouses. I removed any lots, being sold because I was interested in only homes. Land space cannot be negative, and I noticed some negative landspace so I filtered out the rows with negative landscape values. After using other filtering methods, and one-hot encoding, I ended up with the following dataset (sample shown):\n\n\nShow the code\n# Here I am reading in my data\ndf = pd.read_csv(\"./ml_datasets/US Homes Data (For Sale Properties).csv\")\n\n# I filter to only keep data from the state of Texas, specifically for homes that are for sale\ndf = df[(df[\"state\"].isin([\"TX\"])) & (df[\"property_status\"].isin([\"FOR_SALE\"])) & (~df[\"living_space\"].isin([0,1,2,3]))]\n\n# I only look at apartments, condo, manufactured, multi_family, single_family, and townhouse homes\ndf = df[~df['property_type'].isin(['LOT'])]\n\n# I filter out the rows with negative landscape values\ndf = df[~df['land_space'].isin([-10890.0])]\n\n# Here I change acres to square feet in the land_space column\ndf['land_space'] = round(df.apply(lambda row: row['land_space'] * 43560 if row['land_space_unit'] == 'acres' else row['land_space'], axis=1),2)\n\n# Here I make the price_per_unit column more accurate\ndf['price_per_unit'] = round(df['price'] / df['living_space'], 2)\ndf['price_per_unit'] = df.apply(lambda row: round(row['price'] / row['living_space'], 2) if (row['living_space'] &gt; 0) and not pd.isna(row['living_space']) else row['price_per_unit'], axis=1)\n\n# Here I drop the extra columns that I do not need, and can drop now\ndf = df.drop(['property_url','property_id', 'address', 'street_name', 'apartment', 'city', 'state','land_space_unit', 'broker_id', 'property_status',\n              'year_build', 'total_num_units' , 'listing_age', 'RunDate', 'agency_name', 'agent_name', 'agent_phone', 'latitude', 'longitude',\n              'price_per_unit'], axis=1).reset_index(drop=True)\n\n# One-hot encoding\ndf = pd.get_dummies(df, columns=['property_type'], dtype='int')\n\n# Here I change the name of my postcode column to zip_code\ndf.rename(columns={'postcode': 'zip_code'}, inplace=True)\n\ndf.head(10)\n\n\n\n\n\n\n\n\n\nzip_code\nprice\nbedroom_number\nbathroom_number\nliving_space\nland_space\nis_owned_by_zillow\nproperty_type_APARTMENT\nproperty_type_CONDO\nproperty_type_MANUFACTURED\nproperty_type_MULTI_FAMILY\nproperty_type_SINGLE_FAMILY\nproperty_type_TOWNHOUSE\n\n\n\n\n0\n79903\n239500.0\n5.0\n3.0\n1692.0\n6969.6\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n79925\n165000.0\n4.0\n2.0\n1650.0\n12632.4\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n79905\n118000.0\n4.0\n1.0\n1918.0\n11325.6\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n79903\n414700.0\n4.0\n3.0\n3119.0\n15246.0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n79905\n260000.0\nNaN\nNaN\n3267.0\n6000.0\n0\n0\n0\n0\n1\n0\n0\n\n\n5\n79925\n174950.0\n4.0\n2.0\n1800.0\n6098.4\n0\n0\n0\n0\n0\n1\n0\n\n\n6\n79903\n950000.0\n8.0\n4.0\n6100.0\n31363.2\n0\n0\n0\n0\n0\n1\n0\n\n\n7\n79903\n165000.0\n3.0\n1.0\n1305.0\n6098.4\n0\n0\n0\n0\n0\n1\n0\n\n\n8\n79903\n249000.0\n3.0\n3.0\n2790.0\n6098.4\n0\n0\n0\n0\n0\n1\n0\n\n\n9\n79925\n179000.0\n5.0\n2.0\n2112.0\n6098.4\n0\n0\n0\n0\n0\n1\n0",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html#machine-learning-preparation",
    "href": "Machine_Learning/project1.html#machine-learning-preparation",
    "title": "Decoding the Texas Housing Market with XGBoost-Powered Predictive Analytics",
    "section": "Machine Learning Preparation",
    "text": "Machine Learning Preparation\nIn order to finish preparing my dataframe for a machine learning model, I used a dataset provided in github that provides the geometric shapes by zipcode(“State-Zip-Code-GeoJSON”). This dataframe was used to extract the centroid latitude and centroid longitude for the machine learning model I ended up using- XGBoost Regression. I ended up using this dataframe later on to extract the geometry column which contains the geometric shapes that I needed to create a folium choropleth map that shows the accuracy of my model in predicting prices of homes.\n\n\nShow the code\n# Machine learning preparation.\n# Turning the city column to a categorical type\n# This is important for XGBoost to handle the data correctly\n# df['city'] = df['city'].astype('category')\n# ml_df = df.copy()\n\n# Load the GeoJSON shape file for Texas ZIP codes\nurl = \"https://raw.githubusercontent.com/OpenDataDE/State-zip-code-GeoJSON/refs/heads/master/tx_texas_zip_codes_geo.min.json\"\nzip_shapes = gpd.read_file(url)\n\n# Make sure ZIP codes are strings (important for matching)\ndf['zip_code'] = df['zip_code'].astype(str)\nzip_shapes['ZCTA5CE10'] = zip_shapes['ZCTA5CE10'].astype(str)\n\n# Here I do a left join (everything from the ml_df and only matching rows from zip_shapes)\nnew_df = pd.merge(df,zip_shapes, left_on='zip_code', right_on='ZCTA5CE10', how='left')\n\n# Here I drop the extra columns that I do not need\nml_clean = new_df.drop(['STATEFP10', 'ZCTA5CE10', 'GEOID10','CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10',\n                          'AWATER10', 'INTPTLAT10', 'INTPTLON10', 'PARTFLG10'], axis=1)\n\n# Tell GeoPandas which column holds the geometry\nml_clean = gpd.GeoDataFrame(ml_clean, geometry='geometry')\n\n# Check the current coordinate reference system (CRS)\n# print(ml_clean.crs)\n\n# Now I can safely extract spatial features\n\n# Extract spatial features\n# Calculate centroid coordinates in meters\nml_clean['zip_centroid_lon'] = ml_clean.geometry.centroid.x\nml_clean['zip_centroid_lat'] = ml_clean.geometry.centroid.y\n\n# Here I drop the geometry column (not needed for modeling) and city column (one-hot encoding and label encoding are not ideal for this column)\nml_clean = ml_clean.drop(columns=['geometry'])\n\nzip_shapes.head(10)\n\n\n\n\n\n\n\n\n\nSTATEFP10\nZCTA5CE10\nGEOID10\nCLASSFP10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nPARTFLG10\ngeometry\n\n\n\n\n0\n48\n75801\n4875801\nB5\nG6350\nS\n555807428\n6484251\n+31.7345202\n-095.5313809\nN\nPOLYGON ((-95.68072 31.728, -95.68086 31.72807...\n\n\n1\n48\n75839\n4875839\nB5\nG6350\nS\n381863557\n3928555\n+31.6066930\n-095.5833403\nN\nPOLYGON ((-95.66269 31.64586, -95.66242 31.645...\n\n\n2\n48\n78336\n4878336\nB5\nG6350\nS\n126923194\n31624523\n+27.9269126\n-097.1777757\nN\nPOLYGON ((-97.19642 27.91194, -97.19618 27.912...\n\n\n3\n48\n76389\n4876389\nB5\nG6350\nS\n515265783\n10157866\n+33.5126278\n-098.4576268\nN\nMULTIPOLYGON (((-98.37035 33.44176, -98.3782 3...\n\n\n4\n48\n76310\n4876310\nB5\nG6350\nS\n465637930\n24888388\n+33.7990847\n-098.5098400\nN\nPOLYGON ((-98.50723 33.84418, -98.50234 33.844...\n\n\n5\n48\n78011\n4878011\nB5\nG6350\nS\n493710052\n579968\n+28.7909153\n-098.7176338\nN\nPOLYGON ((-98.74144 28.94192, -98.74292 28.942...\n\n\n6\n48\n78114\n4878114\nB5\nG6350\nS\n950244404\n4879176\n+29.1147177\n-098.2086610\nN\nPOLYGON ((-98.0271 29.10656, -98.02895 29.1028...\n\n\n7\n48\n78052\n4878052\nB5\nG6350\nS\n98193838\n285316\n+29.2036790\n-098.7766943\nN\nPOLYGON ((-98.8381 29.22257, -98.83821 29.2268...\n\n\n8\n48\n79371\n4879371\nB5\nG6350\nS\n728089826\n139271\n+33.9526210\n-102.9662131\nN\nMULTIPOLYGON (((-102.4525 34.08832, -102.45252...\n\n\n9\n48\n78650\n4878650\nB5\nG6350\nS\n172543341\n488366\n+30.3038849\n-097.2188991\nN\nPOLYGON ((-97.25978 30.2201, -97.25981 30.2201...",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html#results",
    "href": "Machine_Learning/project1.html#results",
    "title": "Decoding the Texas Housing Market with XGBoost-Powered Predictive Analytics",
    "section": "Results",
    "text": "Results\nIt is important to note that the geometry column gets dropped since this is not needed in the machine learning model. However, as mentioned earlier, before dropping it we derived the centroid longitude and centroid latitude from the geometry column to use in the machine learning model. This column is used again after training my machine learning model in order to demonstrate the accuracy of the results of my machine learning model compared to the test data in a folium choropleth map. Below you will find the clean final dataframe (sample) that was used by my machine learning model . It is important to note that the target variable used for the machine learning model was the log of 1 plus the “price” column (the home “price” column is represented as an “x” in the equation):\n\\(\\text{target column} = \\log\\left(1+x\\right)\\)\nThe reason why I decided to use this as my target column is because the outliers in the price column were causing my folium choropleth legend scale to be heavily skewed, making it unreadable. Therefore, by changing the scale of the target column house price, the legend for each folium choropleth map shows up balanced, making it easy to read.\nThe reason why I ended up going with the XGBoost Regression model is because this model can work with features that contain missing data, and we used the regression model of XGBoost because our target variable is continuous rather than categorical. I decided to use a test size of 0.20 and a random state of 42 when splitting my data for training and when I created the instance of my model in order to help with reproducibility of the test.\nLet’s go over the evaluation metrics I used for my model. The Mean Squared Error (MSE) measures the average of the squared differences between the predicted and actual values (see equation below):\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nThis model is sensitive to outliers. We can see from my results below that the MSE was 0.12, which is low.\nThe Root Mean Squared Error (RMSE) is the square root of the Mean Squared Error (MSE), which means it brings the units back to those of the target variable (the log scale of the home prices column in this case). This metric provides the average error size. Please see the equation below:\n\\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\nWe can see that the Root Mean Squared Error (RMSE) is 0.35. Again, the root mean squared error is low.\nThe Mean Absolute Error (MAE) measures the average error without considering the direction (negative, positive). Please see equation below: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n\\]\nOur Mean Absolute Error is 0.22, which is low.\nFinally we look at the R2 score. This measures the proportion of the variance in the dependent variable (target variable) that is predictable from the independent variables. R2 values fall within the range 0 to 1 (0% to 100%). The closer the number is to 1 the closer it is to a perfect fit. In other words, the model better explains the variability in the dependent variable (target variable). Please see the equation below:_\n\\[\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n\\]\nWe got a result of 0.85 which is overall great.\nBelow you will see the dataframe that was fed to the XGBoost model to train and test the model (X- represents the features. y- represents the target variable). You will notice the change in the scale of the target variable to a log scale. Below that you will see the results of the metrics used to evaluate the model as we have discussed.\n# Drop zipcode for model training\nX = ml_clean.drop(columns=['price'])\ny = ml_clean['price']\n\n# Here I make the target variable a log to improve my machine learning model performance\ny = np.log1p(y)\n\nX_train_full, X_test_full, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Here I save the zipcodes separately before I drop them for modeling\n\n# Zipcodes aligned to my training set\nzip_train = X_train_full['zip_code'].reset_index(drop=True)\n\n# Zipcodes aligned to my test set\nzip_test = X_test_full['zip_code'].reset_index(drop=True)\n\n# Here I remove the zipcodes from the actual training data\nX_train = X_train_full.drop(columns=['zip_code'])\nX_test = X_test_full.drop(columns=['zip_code'])\n\n# create model instance\nxgb = XGBRegressor(n_estimators=350, max_depth=10, learning_rate=.01, random_state=42)\n# fit model\nxgb.fit(X_train, y_train)\n# make predictions\ny_pred = xgb.predict(X_test)\n\n\nShow the code\n# Drop zipcode for model training\nX = ml_clean.drop(columns=['price'])\ny = ml_clean['price']\n\n# Here I make the target variable a log to improve my machine learning model performance\ny = np.log1p(y)\n\nX_train_full, X_test_full, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Here I save the zipcodes separately before I drop them for modeling\n\n# Zipcodes aligned to my training set\nzip_train = X_train_full['zip_code'].reset_index(drop=True)\n\n# Zipcodes aligned to my test set\nzip_test = X_test_full['zip_code'].reset_index(drop=True)\n\n# Here I remove the zipcodes from the actual training data\nX_train = X_train_full.drop(columns=['zip_code'])\nX_test = X_test_full.drop(columns=['zip_code'])\n\n# create model instance\nxgb = XGBRegressor(n_estimators=350, max_depth=10, learning_rate=.01, random_state=42)\n# fit model\nxgb.fit(X_train, y_train)\n# make predictions\ny_pred = xgb.predict(X_test)\n\nprint(\"X-Features:\")\nX.head(10)\n\n\nX-Features:\n\n\n\n\n\n\n\n\n\nzip_code\nbedroom_number\nbathroom_number\nliving_space\nland_space\nis_owned_by_zillow\nproperty_type_APARTMENT\nproperty_type_CONDO\nproperty_type_MANUFACTURED\nproperty_type_MULTI_FAMILY\nproperty_type_SINGLE_FAMILY\nproperty_type_TOWNHOUSE\nzip_centroid_lon\nzip_centroid_lat\n\n\n\n\n0\n79903\n5.0\n3.0\n1692.0\n6969.6\n0\n0\n0\n0\n0\n1\n0\n-106.441936\n31.786182\n\n\n1\n79925\n4.0\n2.0\n1650.0\n12632.4\n0\n0\n0\n0\n0\n1\n0\n-106.363530\n31.797585\n\n\n2\n79905\n4.0\n1.0\n1918.0\n11325.6\n0\n0\n0\n0\n0\n1\n0\n-106.424848\n31.766598\n\n\n3\n79903\n4.0\n3.0\n3119.0\n15246.0\n0\n0\n0\n0\n0\n1\n0\n-106.441936\n31.786182\n\n\n4\n79905\nNaN\nNaN\n3267.0\n6000.0\n0\n0\n0\n0\n1\n0\n0\n-106.424848\n31.766598\n\n\n5\n79925\n4.0\n2.0\n1800.0\n6098.4\n0\n0\n0\n0\n0\n1\n0\n-106.363530\n31.797585\n\n\n6\n79903\n8.0\n4.0\n6100.0\n31363.2\n0\n0\n0\n0\n0\n1\n0\n-106.441936\n31.786182\n\n\n7\n79903\n3.0\n1.0\n1305.0\n6098.4\n0\n0\n0\n0\n0\n1\n0\n-106.441936\n31.786182\n\n\n8\n79903\n3.0\n3.0\n2790.0\n6098.4\n0\n0\n0\n0\n0\n1\n0\n-106.441936\n31.786182\n\n\n9\n79925\n5.0\n2.0\n2112.0\n6098.4\n0\n0\n0\n0\n0\n1\n0\n-106.363530\n31.797585\n\n\n\n\n\n\n\n\n\nShow the code\nprint(\"y-Target Variable:\")\ny.head(10)\n\n\ny-Target Variable:\n\n\n0    12.386313\n1    12.013707\n2    11.678448\n3    12.935313\n4    12.468441\n5    12.072261\n6    13.764218\n7    12.013707\n8    12.425212\n9    12.095147\nName: price, dtype: float64\n\n\n\n\nShow the code\nprint(\"Mean Squared Error (MSE):\", round(mean_squared_error(y_test, y_pred),2))\nrmse = round(np.sqrt(mean_squared_error(y_test, y_pred)),2)\nprint(\"Root Mean Squared Error (RMSE):\", round(rmse,2))\nprint(\"Mean Absolute Error (MAE):\", round(mean_absolute_error(y_test, y_pred),2))\nprint(\"R² Score:\", round(r2_score(y_test, y_pred),2))\n\n# # undo the log1p transformation to get the actual price predictions\n# y_pred = np.expm1(y_pred)\n# y_test = np.expm1(y_test)\n\nresults = pd.DataFrame({\n    'zip_code': zip_test,\n    'prediction': y_pred,\n    'actual_price': y_test.reset_index(drop=True)\n})\n\n# results\n\n# Here I group by zip_code column and calculate the mean of the prediction and actual values\nresults = results.groupby('zip_code')[['prediction', 'actual_price']].mean()\n\n# Round to two decimal places\nresults['actual_price'] = results['actual_price'].round(2)\n\n# Here I change the name of my predicition column and actual price column\nresults = results.rename(columns={'prediction': 'average_price_prediction_log', 'actual_price': 'actual_average_price_log'})\n\nresults = pd.DataFrame(results).reset_index()\n\n# undo the log1p transformation to get the actual price predictions in different columns\nresults['average_price_prediction'] = np.expm1(results['average_price_prediction_log'])\nresults['actual_average_price'] = np.expm1(results['actual_average_price_log'])\n\n\n# Here I do a left join (everything from the ml_df and only matching rows from zip_shapes)\nresults = pd.merge(results,zip_shapes, left_on='zip_code', right_on='ZCTA5CE10', how='left')\n\n# Here I drop the extra columns that I do not need\nresults = results.drop(['STATEFP10', 'ZCTA5CE10', 'GEOID10','CLASSFP10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10',\n                          'AWATER10', 'INTPTLAT10', 'INTPTLON10', 'PARTFLG10'], axis=1)\n\n# Tell GeoPandas which column holds the geometry\nresults = gpd.GeoDataFrame(results, geometry='geometry')\n\n# Simplify geometry to reduce file size (tolerance controls precision)\n# Tolerance controls how much simplification is applied. Higher values = more simplification (more vertices removed).\n# Lower values = more detailed shape retained. In this case, 0.01 is a relatively small simplification, meaning fine detail is mostly preserved.\n# preserve_topology=True ensures that the simplified geometry does not become invalid. Prevents things like self-intersecting polygons, holes merging or disappearing incorrectly, borders of neighboring shapes overlapping or separating when they shouldn’t.\nresults['geometry'] = results['geometry'].simplify(tolerance=0.01, preserve_topology=True)\n\n# results\n\n\nMean Squared Error (MSE): 0.12\nRoot Mean Squared Error (RMSE): 0.35\nMean Absolute Error (MAE): 0.22\nR² Score: 0.85",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html#folium-choropleth---map-visualization-predicted-vs-actual",
    "href": "Machine_Learning/project1.html#folium-choropleth---map-visualization-predicted-vs-actual",
    "title": "Decoding the Texas Housing Market with XGBoost-Powered Predictive Analytics",
    "section": "Folium Choropleth - Map Visualization (Predicted vs Actual)",
    "text": "Folium Choropleth - Map Visualization (Predicted vs Actual)\nNow lets talk about the results in the folium choropleth map. You will notice that the dataset did not provide all the zipcodes in the state of Texas. This is evident by the missing boundaries. However, there was plenty of data available. After obtaining the results of the test data from the XGBoost model, I needed to aggregate the data by grouping by zipcodes and obtaining the average home price (in log scale) for each zipcode. On the map to the left, you will find the average price prediction by the XGBoost model for each zipcode and on the map to the left you will find the actual average price for each zipcode. As you can see, the model does a great job in predicting average home prices, the results are nearly identical to the actual average prices as shown in the map on the right. Now it is difficult to know what the actual home price is of a home if it is measured in a log scale, which is why the predicted and actual average prices are given normally (converted back to dollars) for each zipcode in each map if you hover over any zipcode in each map. That way, you know the actual average price of a home in dollars for each zipcode.\n\n\nShow the code\n# The actual heat map\n# geojson_data = tacos.__geo_interface__  # convert GeoDataFrame to GeoJSON\n# Create a map centered over Texas\n# width=\"35%\", height=\"70%\"\nm1 = folium.Map(location=[31.9686, -99.9018], zoom_start=5.25, tiles=\"openstreetmap\")\n\n# Add the choropleth (color-coded layer)\nfolium.Choropleth(\n    # This is the GeoJSON data that contains the shapes of the ZIP code areas.\n    geo_data=results,\n    # This is the data that I want to visualize.\n    data=results,\n    # Here I specify what columns to use (the column that ties to the geographic shapes and the column with the data to visualize).\n    # The first column is the ZIP code, and the second column is the average price.\n    columns=['zip_code', 'average_price_prediction_log'],\n    # The key_on value should be a string that represents the path in the GeoJSON structure to the property (key) that holds the value you want to match with the data DataFrame.\n    key_on='feature.properties.zip_code',\n    # This sets the color scale used to fill each area.\n    fill_color='plasma',\n    # This sets the transparency of the filled areas.\n    fill_opacity=0.7,\n    # This sets the transparency of the boundary lines between the geographic shapes.\n    line_opacity=0.45,\n    legend_name= \"Average Price Prediction (Log Scale- Smallest to Largest)\"\n).add_to(m1)\n\ntooltip = GeoJson(\n    results,\n    style_function=lambda x: {\n        # Don't add a fill color (we're already coloring with the choropleth)\n        'fillColor': 'transparent',\n        # Hide the border line color\n        'color': 'transparent',\n        # No border line thickness\n        'weight': 0\n    },\n    tooltip=GeoJsonTooltip(\n        # Columns to display in the tooltip\n        fields=['zip_code', 'average_price_prediction'],\n        # What to display as labels for the fields (instead of the column names)\n        aliases=['ZIP Code:', 'Average Price Prediction:'],\n        # Formats numbers using local formatting (e.g. commas in large numbers)\n        localize=True,\n        # Tooltip \"sticks\" to your mouse as you move around that shape. Nice UX\n        sticky=True,\n        # Shows the field names (the aliases you defined).\n        labels=True\n    )\n).add_to(m1)\n\n# The actual heat map\n# geojson_data = tacos.__geo_interface__  # convert GeoDataFrame to GeoJSON\n# Create a map centered over Texas\n\n# width=\"35%\", height=\"70%\"\nm2 = folium.Map(location=[31.9686, -99.9018], zoom_start=5.25, tiles=\"openstreetmap\")\n\n# Add the choropleth (color-coded layer)\nfolium.Choropleth(\n    # This is the GeoJSON data that contains the shapes of the ZIP code areas.\n    geo_data=results,\n    # This is the data that I want to visualize.\n    data=results,\n    # Here I specify what columns to use (the column that ties to the geographic shapes and the column with the data to visualize).\n    # The first column is the ZIP code, and the second column is the average price.\n    columns=['zip_code', 'actual_average_price_log'],\n    # The key_on value should be a string that represents the path in the GeoJSON structure to the property (key) that holds the value you want to match with the data DataFrame.\n    key_on='feature.properties.zip_code',\n    # This sets the color scale used to fill each area.\n    fill_color='plasma',\n    # This sets the transparency of the filled areas.\n    fill_opacity=0.7,\n    # This sets the transparency of the boundary lines between the geographic shapes.\n    line_opacity=0.45,\n    legend_name= \"Actual Average Price (Log Scale - Smallest to Largest)\"\n).add_to(m2)\n\ntooltip = GeoJson(\n    results,\n    style_function=lambda x: {\n        # Don't add a fill color (we're already coloring with the choropleth)\n        'fillColor': 'transparent',\n        # Hide the border line color\n        'color': 'transparent',\n        # No border line thickness\n        'weight': 0\n    },\n    tooltip=GeoJsonTooltip(\n        # Columns to display in the tooltip\n        fields=['zip_code', 'actual_average_price'],\n        # What to display as labels for the fields (instead of the column names)\n        aliases=['ZIP Code:', 'Actual Average Price:'],\n        # Formats numbers using local formatting (e.g. commas in large numbers)\n        localize=True,\n        # Tooltip \"sticks\" to your mouse as you move around that shape. Nice UX\n        sticky=True,\n        # Shows the field names (the aliases you defined).\n        labels=True\n    )\n).add_to(m2)\n\n# Save the maps as separate HTML files\n# m1.save(\"ml_datasets/predicted_price_map.html\")\n# m2.save(\"ml_datasets/actual_price_map.html\")\n\n# Embed maps side-by-side in HTML\nhtml_string = \"\"\"\n    &lt;div style=\"display: flex;\"&gt;\n        &lt;div style=\"width: 50%; height: 100%;\"&gt;\n            &lt;h3&gt;Average Price Prediction (Log Scale)&lt;/h3&gt;\n            {m1}\n        &lt;/div&gt;\n        &lt;div style=\"width: 50%; height: 100%;\"&gt;\n            &lt;h3&gt;Actual Average Price (Log Scale)&lt;/h3&gt;\n            {m2}\n        &lt;/div&gt;\n    &lt;/div&gt;\n\"\"\".format(m1=m1._repr_html_(), m2=m2._repr_html_())\n\nwith open(\"ml_datasets/dual_map.html\", \"w\") as f:\n    f.write(html_string)\n\ndual_map = HTML(html_string)\n\ndual_map\n\n\n\n    \n        \n            Average Price Prediction (Log Scale)\n            Make this Notebook Trusted to load map: File -&gt; Trust Notebook\n        \n        \n            Actual Average Price (Log Scale)\n            Make this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  }
]